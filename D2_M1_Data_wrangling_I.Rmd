#### Data Wrangling Part I{.tabset}
We already did a bit of data wrangling in the df data frame yesterday. Today, we take those concepts to the next level. If you've never heard of the term, data wrangling refers to basically all the steps you have to take to prepare your data for analysis or plotting. Based on the survey you all filled out, this was the most checked topic to learn, so I'll cover this in (painful?) detail.

Before we get started, open the R_training.Rproj project in RStudio, start a Day_2.R script. Then, load the tidyverse package and the wetland dataset we used at the end of yesterday.
```{r, c18, echo=T, eval=F}
library(tidyverse)
wetdat <- read.csv("./data/example_data/ACAD_wetland_data_example.csv")
```

<details open><summary class='drop'>Read in and View Data</summary>
For this training module, we're going to check and clean up the wetland plant data from yesterday. In the dataset, each row represents a species that was recorded in a wetland plot. Multiple sites have been sampled twice, as indicated by the Year column. PctFreq is the % of subplots a species was found in, ranging from 0 to 100. Ave_Cov is the average % cover of the species, ranging from 0 to 100. 

You can view the structure of the <b>wetdat</b> by clicking on the arrow next to the name in the global environment panel. If you click directly on <b>wetdat</b>, a separate window will open to view the data like a spreadsheet. Both of these are acceptable ways to explore data, but I prefer to explore datasets through code, rather than my mouse.

The code below is usually where I start (though I rarely use tail). 
```{r, c20, echo=T, results='hide', eval=FALSE}
head(wetdat) # shows the first 6 lines of the dataframe, including the names of the columns
tail(wetdat[1:3]) # shows last 6 records of first 3 columns in the dataframe
names(wetdat) # shows the column names of the dataframe
table(wetdat$Site_Name, wetdat$Year) # distribution of sites across years
str(wetdat) # shows the type of data in each column
summary(wetdat) # summarize each field based on data type
```
</details>
<br>

<details open><summary class='drop'>Check and fix numeric fields that R reads as a character</summary>
Both the summary and str functions show an issue with the Avg_Cov column: R classified this as a character, but we want it to be numeric. If R classified this as a character, it means that at least one record within this column contains something that isn't a number. If your data frame isn't huge, you can just run View(wetdat) to see the data and scroll/sort until you find the culprit. Strings will typically sort to the top or bottom of an otherwise numeric column.

```{r, c20b, echo=T, eval=FALSE}
View(wetdat)
```

Just by sorting Ave_Cov in the view above, you can see that there are 2 records with <0.1 instead of 0.1. Also, note the row numbers to far left, which are rows 83 and 106 in this case. the Row numbers are assigned at the time you load the data, and are maintained even when you sort in the view. If you sort the data in code, the row numbers are updated. 

Knowing what you just learned, you could open this file in Excel and fix the data, then reload the file in R. But I'm going to do it in code, so that it works any time you add new data to the dataset that might have the same issues. This isn't the slickest approach because I'm pretending that I don't know which record and which symbols are causing the problems. First, I'm going to create a temporary field called Ave_Cov_num that's a numeric version of Ave_Cov.

```{r c20c, echo=T, results='show'}
wetdat$Ave_Cov_num <- as.numeric(wetdat$Ave_Cov) #create a new field that converts Ave_Cov to numeric
```
Note the "NAs introduced by coercion" warning message in your console. This means that when you converted the Ave_Cov to numeric, there was at least 1 record that R didn't read as a number, and it was set to NA in the Ave_Cov_num column. We're going to use that to find and then fix the record(s) that are causing the issue. 

```{r c20d, echo=T, results='show'}
which(is.na(wetdat$Ave_Cov_num)) 
```
The line above is asking which rows in the Ave_Cov_num field are NA, and returns 83 and 106. Now let's use brackets to return the data from those 2 rows.

```{r, c20e, echo=T, results='show'}
wetdat[which(is.na(wetdat$Ave_Cov_num)), ] #all fields
wetdat[which(is.na(wetdat$Ave_Cov_num)), c("Ave_Cov")] # Ave_Cov only
wetdat$Ave_Cov[which(is.na(wetdat$Ave_Cov_num))] # Another way to see Ave_Cov only 
```

In the code above, we're using brackets to subset the wetdat data frame to only return the rows that have NA in Ave_Cov_num. The first line returns all columns in those rows. The second and third lines only show us the Ave_Cov field, which is what we want to fix. Remember that brackets specify the [row, column] of a dataset. If you specify the dataset$column then you only have the row left, and don't need a comma in the brackets. For example: 
```{r, c20f, echo=T, eval=FALSE}
wetdat$Ave_Cov[2] #returns 2nd row in that column
wetdat$Ave_Cov[2, ] #incorrect number of dimensions, because you already specified the Ave_Cov column.
```
Admittedly, subsetting with brackets can get complicated, and I'm not always sure if I need a comma in the bracket or not. If I get the "incorrect number of dimensions" error, I know I did it wrong and need to either add or delete the comma. 

Back to cleaning the data. We found that the records with non-numeric symbols were both "<0.1". We're going to use that information to replace "<0.1" with 0.1, instead of replace all NAs with 0.1 (just in case there are blanks in the data for other reasons). Of course the first thing you should do, when you see <0.1 is to check your raw data, datasheets, etc. to make sure 0.1 is the correct replacement for the <0.1 records. If you don't know what the correct replacement should be, you should probably change the value to NA instead of "0.1". That example is commented out in the code chunk below. 

```{r, c20g, echo=T, results='show'}
wetdat$Ave_Cov[wetdat$Ave_Cov == "<0.1"] <- "0.1"
#wetdat$Ave_Cov[wetdat$Ave_Cov == "<0.1"] <- NA
str(wetdat)
```

You can now View(wetdat) to see if <0.1 is still in the dataset (it shouldn't be). However, notice that Ave_Cov is still considered a chr (text field). You have to manually change it to numeric, but this time, we shouldn't get any NAs introduced by coercion. I'm going to overwrite the Ave_Cov_num field because it's just a temporary field for the purpose of data checking.

```{r, c20h, echo=T, results='show'}
wetdat$Ave_Cov_num <- as.numeric(wetdat$Ave_Cov)
str(wetdat) 
```

Ave_Cov_num is numeric and no NAs were introduced by coercion, so it's safe to overwrite the original Ave_Cov field. The entire workflow from start to finish is below:
```{r, c20h2, echo=F, results='hide'}
wetdat <- read.csv("./data/example_data/ACAD_wetland_data_example.csv")
```

```{r, c20i, echo=T, results='show'}
wetdat$Ave_Cov_num <- as.numeric(wetdat$Ave_Cov)
wetdat$Ave_Cov[which(is.na(wetdat$Ave_Cov_num))] #"<0.1"
wetdat$Ave_Cov[wetdat$Ave_Cov == "<0.1"] <- "0.1"
wetdat$Ave_Cov <- as.numeric(wetdat$Ave_Cov)
wetdat <- subset(wetdat, select = -c(Ave_Cov_num)) #now drop the temp column Ave_Cov_num
str(wetdat) #Ave_Cov is now numeric
table(complete.cases(wetdat$Ave_Cov)) #checks that there aren't any more NAs. 
# If there were NAs, you'd have a column named FALSE with the number of NAs below it.
```
</details>
<br>

<details open><summary class='drop'>Check and fix range of numeric data</summary>
Another check I often do is to make sure there aren't any impossible values. For example, both the Ave_Cov and PctFreq columns should range from 0 to 100. Negative values or values over 100 are errors. Let's see if we find any in the data. Note that we couldn't use the range() function with the original dataset until we fixed the <0.1 issue and converted to numeric.

```{r, c21a, echo=T, results='show'}
range(wetdat$PctFreq, na.rm = TRUE) # 20 to 100- that looks fine.
range(wetdat$Ave_Cov, na.rm = TRUE) # 0.02 to 110.0- no bueno
```
The Ave_Cov column has at least 1 record >100. We're going to find out how many records are >100, then replace them with the correct value. Note also the <b>na.rm = TRUE</b>. That tells R that if there are any blanks in the data, remove them for the range calculation, and only return numbers. If you didn't include na.rm = TRUE, range would return NA NA if there's a blank in the column (more on this in the next section). 

```{r, c21b, echo=T, results='show'}
wetdat[wetdat$Ave_Cov > 100, ]
```
There's only 1 row, row 36, which has an Ave_Cov > 100. In this case, pretend that we went back to the datasheet, and found that there was a data transcription error, and the correct value should be 11. The next line of code fixes the data for that specific record.
```{r, c21c, echo=T, results='show'}
wetdat$Ave_Cov[wetdat$Ave_Cov > 100] <- 11
range(wetdat$Ave_Cov) 
```
Now Ave_Cov values are within the acceptable range of 0 to 100.
</details>
<br>

<details open><summary class='drop'>Finding and fixing blanks (NA)</summary>
As I've mentioned before, R calls blanks in your data NA. R by default is very conservative about how it handles NAs. If you have an NA in an otherwise numeric column and are trying to do a mathematical operation (eg calculate sum, or mean, or run an ANOVA or linear regression), R will most likely return NA as the result. The philosophy behind that is the user must decide how to deal with NAs- either replace them with a logical value, like 0, or remove NAs from the calculation. Every situation is different. You, the user, need to decide the correct way to deal with NAs in your data.

First, let's look to see if there are NAs in the PctFreq and Ave_Cov columns (the main data of interest in the wetdat dataset). 
```{r c21d, echo=T, results='show'}
table(complete.cases(wetdat$Ave_Cov)) #all TRUE
table(complete.cases(wetdat$PctFreq)) #some FALSE
```

While the Ave_Cov colum is all TRUE, the PctFreq has 5 NAs (FALSE in output). Now we need to decide whether the NAs should be changed to 0 or dropped from the dataset. Again, that's up to the user and what you're trying to do. For the purposes here, I'll show you that the results change based on the decision you make.

First, let's do nothing. 
```{r c21e, echo=T, results='show'}
mean(wetdat$PctFreq)
```
NA is returned. R's basically awaiting further instruction. Let's drop NAs from the calculation.
```{r c21f, echo=T, results = 'show'}
mean(wetdat$PctFreq, na.rm = TRUE)
```
Returns 70.36779. Note that na.rm = TRUE is pretty universal argument in R functions. Functions are typically set to na.rm = FALSE, which returns NA if there are blanks in your data. Now let's assume that blanks should actually be 0. 

```{r c21g, echo=T, results='show'}
wetdat$PctFreq[is.na(wetdat$PctFreq)] <- 0
table(complete.cases(wetdat$PctFreq))
mean(wetdat$PctFreq)
```
Now all records in PctFreq are TRUE/complete, and the mean is slightly smaller because the 0s are included.

</details>
<br>

<details open><summary class='drop'>Check and fix character columns</summary>
A common issue with text/character fields is inconsistent naming conventions, for example with plot names. In the wetdat data frame, we have a column called Site_Name, which should be a unique name for each site that starts with a 3-letter code (either SEN or RAM), is followed by a dash, then a 2 digit site number. Let's check to see if all of the Site_Name records follow that. Since we don't have a huge dataset here, we can use the table() function, which will count the number of records in each unique value of a column.
```{r, c21h, echo=T, results='show'}
table(wetdat$Site_Name)
```
A couple of issues show up here. First, we see that there's a site named RAM44 that's missing a dash. We also see that there's one SEN-3, which should be SEN-03. We'll fix both of these below:
```{r, c21i, echo= T, results='show'}
wetdat$Site_Name[wetdat$Site_Name == "RAM44"] <- "RAM-44"
wetdat$Site_Name[wetdat$Site_Name == "SEN-3"] <- "SEN-03"
table(wetdat$Site_Name)
sort(unique(wetdat$Site_Name)) #another way to see all the unique values in a column
```
Problems solved. 
</details>
<br>


<details open><summary class='drop'>Checking your work</summary>
If you plan to calculate summary statistics, like average or standard error, or are doing a lot of filtering, reshaping, etc., to your data, you always want to check your work and make sure you've still got the correct number of sites, individuals, visits, etc. before moving to the next step. Just because you didn't get an error or warning, doesn't mean the code you ran worked. 

In the case of the wetdat, there should be 508 total rows, 7 sites, and three different years. I will show you different ways to check that in the code below.
```{r, c21j, echo=T, results='show'}
dim(wetdat) # reports # rows then # cols
nrow(wetdat) # reports # rows only

length(unique(wetdat$Site_Name)) # number of unique values in Site_Name
length(unique(wetdat$Year)) # number of unique years

table(wetdat$Site_Name, wetdat$Year) # distribution of sites across years
```
</details>
<br>

<hr>