#### Data Wrangling Part I{.tabset}
We already did a bit of data wrangling with the df and wetdat data frames yesterday. Today, we take those concepts to the next level. If you've never heard of the term, data wrangling refers to basically all the steps you have to take to prepare your data for analysis or plotting. Data munging or cleaning are other terms used to describe this process. Based on the survey you all filled out, this was the most checked topic to learn, so I'll cover this in (painful?) detail.

For this training module, we're going to check and clean up the wetland plant data from yesterday. In the dataset, each row represents a species that was recorded in a wetland plot. Multiple sites have been sampled twice, as indicated by the Year column. PctFreq is the % of subplots a species was found in, ranging from 0 to 100. Ave_Cov is the average % cover of the species, ranging from 0 to 100. 

Before we get started, open the R_training.Rproj project in RStudio, start a Day_2.R script. Then, load the tidyverse package and the wetland dataset we used yesterday.
```{r, c18, echo=T, eval=F}
library(tidyverse)
wetdat <- read.csv("./data/example_data/ACAD_wetland_data_example.csv")
```

<details open><summary class='drop'>Explore data structure</summary>
Once you've loaded the wetdat dataset, remember that you can view the structure of the <b>wetdat</b> by clicking on the arrow next to the name in the global environment panel. If you click directly on <b>wetdat</b>, a separate window will open to view the data like a spreadsheet. Both of these are acceptable ways to explore data, but I prefer to explore datasets through code, rather than by mouse.

The code below is usually where I start (though I rarely use tail). 
```{r, c20, echo=T, results='hide', eval=FALSE}
head(wetdat) # shows the first 6 lines of the data frame, including the names of the columns
tail(wetdat[1:3]) # shows last 6 records of first 3 columns in the data frame
names(wetdat) # shows the column names of the data frame
table(wetdat$Site_Name, wetdat$Year) # distribution of sites across years
table(complete.cases(wetdat)) # FALSE counts the number of rows that have an NA in at least 1 column
dim(wetdat) # number of rows (first) and columns (second)
nrow(wetdat) # number of rows (ncol is # columns)
str(wetdat) # shows the type of data in each column
summary(wetdat) # summarize each field based on data type

```
</details>
<br>

<details open><summary class='drop'>Check and fix numeric fields that R reads as a character</summary>
Both the summary and str functions show an issue with the Avg_Cov column: R classified this as a character, but we want it to be numeric. If R classified this as a character, it means that at least one record within this column contains something that isn't a number. If your data frame isn't huge, you can just run View(wetdat) to see the data and scroll/sort until you find the culprit. Strings will typically sort to the top or bottom of an otherwise numeric column.

```{r, c20b, echo=T, eval=FALSE}
View(wetdat)
```
Just by sorting Ave_Cov in the view above, you can see that there are 2 records with <0.1 instead of 0.1. Also, note the row numbers to far left, which are rows 83 and 106 in this case. the Row numbers are assigned at the time you load the data, and are maintained even when you sort in the view. If you sort the data in code, the row numbers are updated. 

Knowing what you just learned, you could open this file in Excel and fix the data, then reload the file in R. But I'm going to do it in code, so that it works any time you add new data to the dataset that might have the same issues. This isn't the slickest approach because I'm pretending that I don't know which record and which symbols are causing the problems. First, I'm going to create a temporary field called Ave_Cov_num that's a numeric version of Ave_Cov.

```{r c20c, echo=T, results='show'}
wetdat$Ave_Cov_num <- as.numeric(wetdat$Ave_Cov) #create new field that converts Ave_Cov to numeric
```
Note the "NAs introduced by coercion" warning message in your console. This means that when you converted the Ave_Cov to numeric, there was at least 1 record that R didn't read as a number, and it was set to NA (blank) in the Ave_Cov_num column. We're going to use that to find and then fix the record(s) that are causing the issue. 

```{r c20d, echo=T, results='show'}
which(is.na(wetdat$Ave_Cov_num)) 
```
The line above is asking which rows in the Ave_Cov_num field are NA, and returns 83 and 106. Now let's use brackets to return the data from those 2 rows.

```{r, c20e, echo=T, results='show'}
wetdat[which(is.na(wetdat$Ave_Cov_num)), ] # all columns with NA in Ave_Cov_num
wetdat[which(is.na(wetdat$Ave_Cov_num)), "Ave_Cov"] # Ave_Cov only
wetdat$Ave_Cov[which(is.na(wetdat$Ave_Cov_num))] # Another way to see Ave_Cov only 
```

In the code above, we're using brackets to subset the wetdat data frame to only return the rows that have NA in Ave_Cov_num. The first line returns all columns in those rows. The second and third lines only show us the Ave_Cov field, which is what we want to fix. Remember that brackets specify the [row, column] of a data frame. If you specify the dataframe$column then you only have the row left, and don't need a comma in the brackets. Admittedly, subsetting with brackets can get complicated, and I'm not always sure if I need a comma in the bracket or not. If I get the "incorrect number of dimensions" error, I know I did it wrong and need to either add or delete the comma. 

Back to cleaning the data. We found that the records with non-numeric symbols were both "<0.1". We're going to use that information to replace "<0.1" with 0.1, instead of replace all NAs with 0.1 (just in case there are blanks in the data for other reasons). 

Of course the first thing you should do, when you see <0.1 is to check your raw data, datasheets, etc. to make sure 0.1 is the correct replacement for the <0.1 records. If you don't know what the correct replacement should be, you should probably change the value to NA instead of "0.1". That example is commented out in the code chunk below. 

```{r, c20g, echo=T, results='show'}
wetdat$Ave_Cov[wetdat$Ave_Cov == "<0.1"] <- "0.1"
#wetdat$Ave_Cov[wetdat$Ave_Cov == "<0.1"] <- NA
str(wetdat)
```

You can now View(wetdat) to see if <0.1 is still in the data frame (it shouldn't be). However, notice that Ave_Cov is still considered a chr (text field). Once R assigns a data type to a column, you have to manually change it to something different. Now that we fixed <0.1, we shouldn't get any NAs introduced by coercion when we convert to numeric. I'm going to overwrite the Ave_Cov_num field because it's just a temporary field for the purpose of data checking.

```{r, c20h, echo=T, results='show'}
wetdat$Ave_Cov_num <- as.numeric(wetdat$Ave_Cov)
str(wetdat) 
```
Ave_Cov_num is now numeric and no NAs were introduced by coercion. It's now safe to overwrite the original Ave_Cov field. The entire workflow from start to finish is below:
```{r, c20h2, echo=F, results='hide'}
wetdat <- read.csv("./data/example_data/ACAD_wetland_data_example.csv")
```
```{r, c20i, echo=T, results='show'}
wetdat$Ave_Cov_num <- as.numeric(wetdat$Ave_Cov)
wetdat$Ave_Cov[which(is.na(wetdat$Ave_Cov_num))] # which values in Ave_Cov are forcing Ave_Cov_num to be NA
wetdat$Ave_Cov[wetdat$Ave_Cov == "<0.1"] <- "0.1"
wetdat$Ave_Cov <- as.numeric(wetdat$Ave_Cov)
wetdat <- subset(wetdat, select = -c(Ave_Cov_num)) #now drop the temp column Ave_Cov_num
str(wetdat) #Ave_Cov is now numeric
table(complete.cases(wetdat$Ave_Cov)) #checks that there aren't any more NAs. 
# If there were NAs, you'd have a column named FALSE with the number of NAs below it.
```
</details>
<br>

<details open><summary class='drop'>Check and fix range of numeric data</summary>
Another check I often do is to make sure there aren't any impossible values. For example, both the Ave_Cov and PctFreq columns are percents that should range from 0 to 100. Negative values or values over 100 are errors. Let's see if we find any in the data.Note that we couldn't use the range() function on Ave_Cov with the original data frame until we fixed the <0.1 issue and converted to numeric.


```{r, c21a, echo=T, results='show'}
range(wetdat$PctFreq, na.rm = TRUE) # 20 to 100- that looks fine.
range(wetdat$Ave_Cov, na.rm = TRUE) # 0.02 to 110.0- no bueno
```

The Ave_Cov column has at least 1 record >100. Now we're going to find out how many records are >100, then replace them with the correct value. 

Note also the <b>na.rm = TRUE</b>. That tells R that if there are any blanks in the data, remove them for the range calculation, and only return numbers. If you didn't include na.rm = TRUE, range would return NA NA if there's a blank in the column (more on this in the next section). 

```{r, c21b, echo=T, results='show'}
wetdat[wetdat$Ave_Cov > 100, ] # If Ave_Cov is still a chr, this filter won't work
```
There's only 1 row, row 36, which has an Ave_Cov > 100 for Arethusa bulbosa in SEN-01. For our purposes, pretend that we went back to the datasheet, and found that there was a data transcription error, and the correct value should be 11, not 110. The next line of code fixes the data for that specific record.

```{r, c21c, echo=T, results='show'}
wetdat$Ave_Cov[wetdat$Ave_Cov > 100] <- 11
range(wetdat$Ave_Cov) 
```
Now Ave_Cov values are within the acceptable range of 0 to 100.
</details>
<br>

<details open><summary class='drop'>Finding and fixing blanks (NA)</summary>
As I've mentioned before, R calls blanks in your data NA. R by default is very conservative about how it handles NAs. If you have an NA in an otherwise numeric column and are trying to do a mathematical operation (eg calculate sum or mean), R will most likely return NA as the result. The philosophy behind that is the user must decide how to deal with NAs- either replace them with a logical value, like 0, or remove them from the calculation. Every situation is different. You, the user, will need to decide the correct way to deal with NAs in your data.

First, let's look to see if there are NAs in the PctFreq and Ave_Cov columns (the main data of interest in the wetdat dataset). 
```{r c21d, echo=T, results='show'}
table(complete.cases(wetdat$Ave_Cov)) #all TRUE
table(complete.cases(wetdat$PctFreq)) #some FALSE
```

While the Ave_Cov colum is all TRUE, the PctFreq has 5 NAs (FALSE in output). Now we need to decide whether the NAs should be changed to 0 or dropped from the dataset. Again, that's up to the user and what you're trying to do. For the purposes here, I'll show you how the results change based on the decision you make.

First, let's do nothing. 
```{r c21e, echo=T, results='show'}
mean(wetdat$PctFreq)
```
NA is returned. R's basically awaiting further instruction before it's willing to return a value. Let's drop NAs from the calculation.
```{r c21f, echo=T, results = 'show'}
mean(wetdat$PctFreq, na.rm = TRUE)
```
Returns 70.36779. Note that na.rm = TRUE is pretty universal argument in R functions. Functions are typically set to na.rm = FALSE, which returns NA if there are blanks in your data. Changing the argument to na.rm = TRUE will drop NAs from the function. 

Now let's assume that blanks should actually be 0. 
```{r c21g, echo=T, results='show'}
wetdat$PctFreq[is.na(wetdat$PctFreq)] <- 0
table(complete.cases(wetdat$PctFreq))
mean(wetdat$PctFreq)
```
Now all records in PctFreq are TRUE/complete, and the mean is slightly smaller because the 0s are included.
</details>
<br>

<details open><summary class='drop'>Check and fix character columns</summary>
A common issue with text/character fields is inconsistent naming conventions, especially if data are being stored in a spreadsheet instead of a database with standardized look up fields. In the wetdat data frame, we have a column called Site_Name, which should be a unique name for each site that starts with a 3-letter code (either SEN or RAM), is followed by a dash, then a 2 digit site number. Let's check to see if all of the Site_Name records follow that. Since we don't have a huge dataset here, we can use the table() function, which will count the number of records in each unique value of a column.
```{r, c21h, echo=T, results='show'}
table(wetdat$Site_Name)
```
A couple of issues show up here. First, we see that there's a site named RAM44 that's missing a dash. We also see that there's one SEN-3, which should be SEN-03. 

Another way to do this, if you had a large dataset, would be to check if there are any Site_Name records that are not length 6 (eg RAM-01 is 6 digits) using the nchar() function, which counts the number of characters in a record. 

```{r, c21j, echo=T, results='show'}
wetdat$Site_Name[which(nchar(wetdat$Site_Name)!=6)] #

```

You can also check if any Site_Name records are missing a dash using the grepl function. The grepl function is a string matching 
```{r, c21k, echo=T, results='show'}
wetdat$Site_Name[which(!grepl("-", wetdat$Site_Name))]
```

Now to fix and then check both of the issues:
```{r, c21i, echo= T, results='hide', eval=T}
wetdat$Site_Name[wetdat$Site_Name == "RAM44"] <- "RAM-44"
wetdat$Site_Name[wetdat$Site_Name == "SEN-3"] <- "SEN-03"
wetdat$Site_Name[which(!grepl("-", wetdat$Site_Name))]
wetdat$Site_Name[which(nchar(wetdat$Site_Name)!=6)]
table(wetdat$Site_Name)
sort(unique(wetdat$Site_Name)) #another way to see all the unique values in a column
```
Problems solved (character(0) means no records returned). Note that there are other more powerful solutions using functions like gsub, but they require more upfront coding and understand of regular expressions in Perl to work. I only use them when the solution above doesn't work. If you ever need to know more about that, a decent intro is <a href="https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html">here</a>. I provide examples on how to use regular expressions for pattern matching in the answers section, but I'm not going to go down that rabbit hole during training (I barely understand them myself).

</details>
<br>

<details open><summary class='drop'>Checking your work</summary>
If you plan to calculate summary statistics, like average or standard error, or are doing a lot of filtering, reshaping, etc. to your data, you always want to check your work and make sure you've still got the correct number of sites, individuals, visits, etc. before moving on to the next step. Just because you didn't get an error or warning, doesn't mean the code you ran worked! 

In the case of the wetdat, there should be 508 total rows, 7 sites, and three different years. I will show you different ways to check that in the code below. I also often skim the unique values in each field for anything weird (hence the sort(unique()) line).
```{r, c21l, echo=T, results='hide'}
dim(wetdat) # reports # rows then # cols
nrow(wetdat) # reports # rows only

length(unique(wetdat$Site_Name)) # number of unique values in Site_Name
length(unique(wetdat$Year)) # number of unique years

table(wetdat$Site_Name, wetdat$Year) # distribution of sites across years

sort(unique(wetdat$Common)) # see all the common names in the dataset
```

<p class='ques'>Question 1: There's a Latin name that needs to be fixed (hint look for non-letter symbols). How would you find that record?</p>

<p class='ques'>Question 2: Now that you've found it, fix it, and check that you fixed it.</p>

</details>
<br>

<hr>