#### Importing Data into R{.tabset}

For the training modules that follow, we're going to work with a larger dataset that contains some wetland plant data. First, import the data by running the code chunk below. 

```{r, c19, echo=T, results='hide'}
wetdat <- read.csv("./data/example_data/ACAD_wetland_data_example.csv")
```

After running the code above, you should see a new object in your global environment named <b>wetdat</b> You can view the structure and the data by clicking on the arrow next to the name in the global environment panel. If you click directly on <b>wetdat</b>, a separate window will open to view the data like a spreadsheet. 

<h1>Add how to load excel here</h1>

#### Subsetting Data{.tabset}
<details open><summary class='drop'>Subsetting Data</summary>
Technically, we already did some subsetting when we were trying to identify and fix problems in the df data frame. Now we're going to do it more intentionally. Subsetting is a generic term for reducing the size of a dataset, either by reducing the number of rows or colums in the dataset. Filtering typically means reducing rows. Selecting usually means reducing columns. 

Using the wetdat data frame from above, let's remove all species from the dataset that are considered protected from public (eg sensitive species we don't want to publish the location to). There's a logical column called Protected in the dataset that we're going to use to filter out rows when Protected is TRUE. Let's also simplify the data frame to only include the columns we care about.
```{r, c22, echo=T, results='show'}
# Filter to remove protected species
wetdat_pub <- wetdat[wetdat$Protected == FALSE, ]
nrow(wetdat) #508 rows in original dataset
nrow(wetdat_pub) #499 rows in filtered dataset
table(wetdat_pub$Protected) # Only Protected = FALSE

# Select only fields you want
wetdat_pub <- wetdat_pub[ , c("Site_Name", "Site_Type", "Latin_Name", "Year", "Ave_Cov", "Invasive", "Protected")]
names(wetdat_pub)

# Previous steps can all happen in 1 line of code:
wetdat_pub <- wetdat[wetdat$Protected == FALSE, 
                     c("Site_Name", "Site_Type", "Latin_Name", "Year", "Ave_Cov", "Invasive", "Protected")]

nrow(wetdat_pub) #499 rows
names(wetdat_pub) # Only includes fields we want
table(wetdat_pub$Protected) #All FALSE
```
Note the use of == to subset. In R, when you're trying to match against something (like Protected == FALSE, or Site_Name == "RAM44"), you use double == signs. When assigning something to an object (like mutate(newcolum = x + y)), you use a single = sign. 

If you want to pull out all records that are NOT equal to a value, you use !=. That is, in R, ! means NOT. For example Protected != TRUE means to pull in records where Protected is NOT TRUE (ie FALSE).

Now the tidyverse way do in the previous code chunk using the select and filter command and pipes (%>%). The select function is used for columns and the filter command is used for rows. Notice the lack of quotes for the column names. One of the benefits of the tidyverse packages is that you don't have to quote column names, whereas you do in base R. It saves typing time and was a conscious decision of the tidyverse developers.
```{r, c22b, echo=T, results='hide'}
wetdat_tidy <- wetdat %>% filter(Protected == FALSE) %>% 
                          select(Site_Name, Site_Type, Latin_Name, Year, Ave_Cov, Invasive, Protected)
                          # Any column name not mentioned in the select is dropped

nrow(wetdat_tidy)
names(wetdat_tidy)
table(wetdat_tidy$Protected)

# Even more efficient way to code this:
wetdat_tidy <- wetdat %>% filter(Protected == FALSE) %>% select(-Common, -PctFreq)
```

Note the %>%, called a "pipe". The way to read what the pipe is doing is it takes the wetdat data frame, pipes it into the filter function, which drops protected species, then that newly filtered dataset is piped into the select function, and the Common and PctFreq columns are dropped. Also notice that column names aren't quoted in tidy approach. That's one of the big benefits of tidyverse functions. Not having to quote column names saves you time coding. It's called non-standard evaluation. It actually causes problems when you write your own functions, but we won't get into that during this training.
</details>
<br>

<details open><summary class='drop'>Filtering with multiple values</summary>
This section shows you how to filter rows based on multiple possible values. Using the wetland data again, let's say we only want to look at the data from 2 sites: SEN-01 and SEN-03. To do that, we create a list of strings we want to match and tell R to take any record that matches any of the strings in that list. 

```{r, c25, echo=T, results='hide'}
# Base R
wetdat_2sites <- wetdat[wetdat$Site_Name %in% c("SEN-01", "SEN-03"), ] 
nrow(wetdat_2sites)
table(wetdat_2sites$Site_Name)

# Tidyverse
wetdat_2tidy <- wetdat %>% filter(Site_Name %in% c("SEN-01", "SEN-03"))
nrow(wetdat_2tidy)
table(wetdat_2tidy$Site_Name)


```
The nice thing about using %in% is that it only pulls records that match exactly one of the values you give it, and you don't have to worry about it also including NAs. 

<p class="ques">Question 1: How would you select only the plots in VAFO and HOFU from the loc table?</p>

<p class="ques">Question 2: How many plots are in VAFO and HOFU combined (based on loc table)?</p>
</details>
</br>
<details open><summary class='drop'>Sorting Data</summary>
Next topic is sorting data in R, which you can do alphabetically or numerically. Typically when you read in a dataset in R, the order is exactly the order of that in the original file. There are multiple ways to sort data in R. Let's start by sorting the loc table by Unit_ID and Plot_Number. 
```{r, c26, echo=T, results='hide'}
# head(loc2) # check original before sorting

# # Base R- I always forget the exact code and have to look it up.
# loc2_sort <- loc2[order(loc2$Unit_ID, loc2$Plot_Number), ]
# head(loc2_sort)
# 
# # Sort in reverse
# loc2_sort_desc <- loc2[order(desc(loc2$Unit_ID), -loc2$Plot_Number), ]
# head(loc2_sort_desc)
# # desc is for text, - is for numbers
# 
# # Tidyverse version
# loc2_sort_tidy <- loc2 %>% arrange(Unit_ID, Plot_Number)
# head(loc2_sort_tidy)
# 
# #Tidyverse reverse sort
# loc2_sort_tidy_rev <- loc2 %>% arrange(desc(Unit_ID), desc(Plot_Number))
# head(loc2_sort_tidy_rev)
```
</details>
</br>
<details open><summary class='drop'>Summarizing Data</summary>
The next topic is summarizing data using group_by() and summarize() functions from dplyr. The group_by() function allows you to specify the columns you want to summarize for each like row in the column. For example grouping by plot number, and summing the basal area for all trees on each plot. Another example is grouping by park (Unit_ID), and calculating the average tree basal area in a park. 

To demonstrate how this works, let's load in a test dataset.
```{r, c27, echo=T, results='hide'}
regdf <- read.csv('./data/example_data/Regen_test_data.csv')
head(regdf)
```
The regdf dataframe has seedling and sapling densities for the latest visit in each plot in HOFU, RICH and VAFO parks. The stock column is the stocking index, which is another measure of regeneration abundance. Let's say we want to calculate the mean and standard error of each of the 3 regeneration metrics at the park level (ie Unit_Code). 

```{r, c28, echo=T, results='hide'}

reg_by_park <- regdf %>% group_by(Unit_Code) %>% 
                         summarize(avg_seed_dens = mean(seed_den_m2),
                                   avg_sap_dens = mean(sap_den_m2),
                                   avg_stock = mean(stock),
                                   se_seed_dens = sd(seed_den_m2)/sqrt(n()),
                                   se_sap_dens = sd(sap_den_m2)/sqrt(n()),
                                   se_stock = sd(stock)/sqrt(n()),
                                   numplots = n()
                                   )
```
```{r, c29, echo=T}
print(reg_by_park)
```

In the code chunk above, we started with a seed_den_m2, sap_den_m2, and stock record for each plot (regdf). We then grouped by Unit_Code, which is the 4-letter park code, to come up with a mean, min, max and SE for each regen metric at the park level. Because there were only 3 parks in the dataframe, the resulting summary returned 3 results. Note that n() counts the number of rows within each group. By calculating sd(metric)/sqrt(n), we get the standard error.

<p class='ques'>Question 6: How would you calculate the min and max for seed_den_m2?</p>
</details>
</br>
